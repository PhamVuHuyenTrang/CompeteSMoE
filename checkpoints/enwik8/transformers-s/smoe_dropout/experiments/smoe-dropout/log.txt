Training Parameters:
 {'batch_size': 48, 'batch_split': 2, 'nb_batches_per_iter': 1000, 'nb_iter': 60, 'checkpoint_path': 'checkpoints/enwik8/transformers-s/smoe_dropout/smoe-dropout.pt', 'full_eval_mode': False}
2023-11-26 20:23:30.486773
DataParallel(
  (module): TransformerSeq(
    (in_emb): Embedding(205, 264)
    (out_emb): Linear(in_features=264, out_features=205, bias=True)
    (layers): ModuleList(
      (0-5): 6 x TransformerSeqLayer(
        (attn): MultiHeadSeqAttention(
          (attn): SeqAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (proj_query): Linear(in_features=264, out_features=264, bias=False)
          (proj_out): Linear(in_features=264, out_features=264, bias=False)
          (proj_val): Linear(in_features=264, out_features=264, bias=False)
          (proj_key): Linear(in_features=264, out_features=264, bias=False)
        )
        (smoe): CustomizedMoEPositionwiseFF(
          (gate): CustomNaiveGate_Balance_SMoE(
            (gate): Linear(in_features=264, out_features=16, bias=True)
          )
          (experts): _Expert(
            (htoh4): FMoELinear(num_expert=16, in_features=264,         out_features=264, bias=True, rank=0)
            (h4toh): FMoELinear(num_expert=16, in_features=264,         out_features=264, bias=True, rank=0)
            (activation): Sequential(
              (0): ReLU()
              (1): Dropout(p=0.1, inplace=False)
            )
          )
          (layer_norm): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
Total of Parameters: 15319165
Total of Trainable Parameters: 15319165
