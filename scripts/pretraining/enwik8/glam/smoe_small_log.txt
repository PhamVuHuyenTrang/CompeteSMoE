Training ...
data_params:	 {'data_path': '/home/gtruong/Project/ICML2/data/enwik8'}
model_params:	 {'hidden_size': 264, 'inner_hidden_size': 264, 'nb_layers': 3, 'block_size': 512, 'nb_heads': 8, 'attn_span': 2048, 'dropout': 0.1, 'architecture': 'sgsfsgsfsgsf', 'base_arch': 'glam', 'load_balance': 0.01, 'gate_name': 'smoe'}
optim_params:	 {'lr': 0.0007, 'momentum': 0.9, 'optim': 'adam', 'lr_warmup': 3000, 'grad_clip': 0}
trainer_params:	 {'batch_size': 48, 'batch_split': 2, 'nb_batches_per_iter': 60000, 'nb_iter': 1, 'checkpoint_path': 'checkpoints/enwik8/glam-smoe-s/smoe.pt', 'full_eval_mode': False}
adapt_span_params:	 {'adapt_span_enabled': False, 'adapt_span_loss': 0, 'adapt_span_ramp': 32, 'adapt_span_init': 0, 'adapt_span_cache': False}
Loading an existing corpus file from /home/gtruong/Project/ICML2/data/enwik8/corpus.pt
sgsfsgsfsgsf
Evaluation ...
data_params:	 {'data_path': '/home/gtruong/Project/ICML2/data/enwik8'}
model_params:	 {'hidden_size': 264, 'inner_hidden_size': 264, 'nb_layers': 3, 'block_size': 512, 'nb_heads': 8, 'attn_span': 2048, 'dropout': 0.1, 'architecture': 'sgsfsgsfsgsf', 'base_arch': 'glam', 'load_balance': 0.01, 'gate_name': 'smoe'}
optim_params:	 {'lr': 0.0007, 'momentum': 0.9, 'optim': 'adam', 'lr_warmup': 3000, 'grad_clip': 0}
trainer_params:	 {'batch_size': 8, 'batch_split': 2, 'nb_batches_per_iter': 60000, 'nb_iter': 1, 'checkpoint_path': 'checkpoints/enwik8/glam-smoe-s/smoe.pt', 'full_eval_mode': True}
adapt_span_params:	 {'adapt_span_enabled': False, 'adapt_span_loss': 0, 'adapt_span_ramp': 32, 'adapt_span_init': 0, 'adapt_span_cache': False}
Loading an existing corpus file from /home/gtruong/Project/ICML2/data/enwik8/corpus.pt
sgsfsgsfsgsf
Training ...
data_params:	 {'data_path': '/home/gtruong/Project/ICML2/data/enwik8'}
model_params:	 {'hidden_size': 264, 'inner_hidden_size': 264, 'nb_layers': 3, 'block_size': 512, 'nb_heads': 8, 'attn_span': 2048, 'dropout': 0.1, 'architecture': 'sgsfsgsfsgsf', 'base_arch': 'glam', 'load_balance': 0.01, 'gate_name': 'smoe'}
optim_params:	 {'lr': 0.0007, 'momentum': 0.9, 'optim': 'adam', 'lr_warmup': 3000, 'grad_clip': 0}
trainer_params:	 {'batch_size': 48, 'batch_split': 2, 'nb_batches_per_iter': 60000, 'nb_iter': 1, 'checkpoint_path': 'checkpoints/enwik8/glam-smoe-s/smoe.pt', 'full_eval_mode': False}
adapt_span_params:	 {'adapt_span_enabled': False, 'adapt_span_loss': 0, 'adapt_span_ramp': 32, 'adapt_span_init': 0, 'adapt_span_cache': False}
Loading an existing corpus file from /home/gtruong/Project/ICML2/data/enwik8/corpus.pt
sgsfsgsfsgsf
Evaluation ...
data_params:	 {'data_path': '/home/gtruong/Project/ICML2/data/enwik8'}
model_params:	 {'hidden_size': 264, 'inner_hidden_size': 264, 'nb_layers': 3, 'block_size': 512, 'nb_heads': 8, 'attn_span': 2048, 'dropout': 0.1, 'architecture': 'sgsfsgsfsgsf', 'base_arch': 'glam', 'load_balance': 0.01, 'gate_name': 'smoe'}
optim_params:	 {'lr': 0.0007, 'momentum': 0.9, 'optim': 'adam', 'lr_warmup': 3000, 'grad_clip': 0}
trainer_params:	 {'batch_size': 8, 'batch_split': 2, 'nb_batches_per_iter': 60000, 'nb_iter': 1, 'checkpoint_path': 'checkpoints/enwik8/glam-smoe-s/smoe.pt', 'full_eval_mode': True}
adapt_span_params:	 {'adapt_span_enabled': False, 'adapt_span_loss': 0, 'adapt_span_ramp': 32, 'adapt_span_init': 0, 'adapt_span_cache': False}
Loading an existing corpus file from /home/gtruong/Project/ICML2/data/enwik8/corpus.pt
sgsfsgsfsgsf
Training ...
data_params:	 {'data_path': '/home/gtruong/Project/ICML2/data/enwik8'}
model_params:	 {'hidden_size': 264, 'inner_hidden_size': 264, 'nb_layers': 3, 'block_size': 512, 'nb_heads': 8, 'attn_span': 2048, 'dropout': 0.1, 'architecture': 'sgsfsgsfsgsf', 'base_arch': 'glam', 'load_balance': 0.01, 'gate_name': 'smoe'}
optim_params:	 {'lr': 0.0007, 'momentum': 0.9, 'optim': 'adam', 'lr_warmup': 3000, 'grad_clip': 0}
trainer_params:	 {'batch_size': 48, 'batch_split': 2, 'nb_batches_per_iter': 60000, 'nb_iter': 1, 'checkpoint_path': 'checkpoints/enwik8/glam-smoe-s/smoe.pt', 'full_eval_mode': False}
adapt_span_params:	 {'adapt_span_enabled': False, 'adapt_span_loss': 0, 'adapt_span_ramp': 32, 'adapt_span_init': 0, 'adapt_span_cache': False}
Loading an existing corpus file from /home/gtruong/Project/ICML2/data/enwik8/corpus.pt
sgsfsgsfsgsf
TransformerSeq(
  (in_emb): Embedding(205, 264)
  (out_emb): Linear(in_features=264, out_features=205, bias=True)
  (layers): ModuleList(
    (0): TransformerSeqLayer(
      (attn): MultiHeadSeqAttention(
        (attn): SeqAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (proj_query): Linear(in_features=264, out_features=264, bias=False)
        (proj_out): Linear(in_features=264, out_features=264, bias=False)
        (proj_val): Linear(in_features=264, out_features=264, bias=False)
        (proj_key): Linear(in_features=264, out_features=264, bias=False)
      )
      (smoe): CustomizedMoEPositionwiseFF(
        (gate): CustomNaiveGate_Balance_SMoE(
          (gate): Linear(in_features=264, out_features=16, bias=True)
        )
        (experts): _Expert(
          (htoh4): FMoELinear(num_expert=16, in_features=264,         out_features=264, bias=True, rank=0)
          (h4toh): FMoELinear(num_expert=16, in_features=264,         out_features=264, bias=True, rank=0)
          (activation): Sequential(
            (0): ReLU()
            (1): Dropout(p=0.1, inplace=False)
          )
        )
        (layer_norm): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
      (norm3): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerSeqLayer(
      (attn): MultiHeadSeqAttention(
        (attn): SeqAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (proj_query): Linear(in_features=264, out_features=264, bias=False)
        (proj_out): Linear(in_features=264, out_features=264, bias=False)
        (proj_val): Linear(in_features=264, out_features=264, bias=False)
        (proj_key): Linear(in_features=264, out_features=264, bias=False)
      )
      (ff): FeedForwardLayer(
        (fc1): Linear(in_features=264, out_features=264, bias=True)
        (fc2): Linear(in_features=264, out_features=264, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
      (norm3): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
    )
    (2-3): 2 x TransformerSeqLayer(
      (attn): MultiHeadSeqAttention(
        (attn): SeqAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (proj_query): Linear(in_features=264, out_features=264, bias=False)
        (proj_out): Linear(in_features=264, out_features=264, bias=False)
        (proj_val): Linear(in_features=264, out_features=264, bias=False)
        (proj_key): Linear(in_features=264, out_features=264, bias=False)
      )
      (norm1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
      (norm3): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerSeqLayer(
      (attn): MultiHeadSeqAttention(
        (attn): SeqAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (proj_query): Linear(in_features=264, out_features=264, bias=False)
        (proj_out): Linear(in_features=264, out_features=264, bias=False)
        (proj_val): Linear(in_features=264, out_features=264, bias=False)
        (proj_key): Linear(in_features=264, out_features=264, bias=False)
      )
      (smoe): CustomizedMoEPositionwiseFF(
        (gate): CustomNaiveGate_Balance_SMoE(
          (gate): Linear(in_features=264, out_features=16, bias=True)
        )
        (experts): _Expert(
          (htoh4): FMoELinear(num_expert=16, in_features=264,         out_features=264, bias=True, rank=0)
          (h4toh): FMoELinear(num_expert=16, in_features=264,         out_features=264, bias=True, rank=0)
          (activation): Sequential(
            (0): ReLU()
            (1): Dropout(p=0.1, inplace=False)
          )
        )
        (layer_norm): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
      (norm3): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerSeqLayer(
      (attn): MultiHeadSeqAttention(
        (attn): SeqAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (proj_query): Linear(in_features=264, out_features=264, bias=False)
        (proj_out): Linear(in_features=264, out_features=264, bias=False)
        (proj_val): Linear(in_features=264, out_features=264, bias=False)
        (proj_key): Linear(in_features=264, out_features=264, bias=False)
      )
      (ff): FeedForwardLayer(
        (fc1): Linear(in_features=264, out_features=264, bias=True)
        (fc2): Linear(in_features=264, out_features=264, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (norm1): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
      (norm3): LayerNorm((264,), eps=1e-05, elementwise_affine=True)
    )
  )
)
nb_parameters=6.63M
